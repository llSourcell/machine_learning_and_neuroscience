{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscience and Machine Learning\n",
    "\n",
    "## Demo - An interactive network visualization tool for exploring functional brain connectivity \n",
    "\n",
    "![alt text](https://i.imgur.com/ZV4kGlu.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- History of Neuroscience in AI\n",
    "- Contemporary Neuroscience in AI\n",
    "- Future of Neuroscience in AI\n",
    "\n",
    "## The History of Neuroscience in AI\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/0*vM6TPG8w6S54DGdC.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- At the dawn of the computer age, AI research and neuroscience were completely intertwined\n",
    "- Early pioneers straddled both fields\n",
    "- And the collaborations between disciplines where highly productive\n",
    "- Let's look at some examples\n",
    "\n",
    "### Examples\n",
    "\n",
    "#### Alan Turing \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/thea-171021150200/95/the-artificial-intelligence-ecosystem-9-638.jpg?cb=1508598480 \"Logo Title Text 1\")\n",
    "\n",
    "###### \"We are not interested in the fact that the brain has the consistency of cold porridge\" (Turing 1952)\n",
    "\n",
    "- Said the key to understanding the brain is not in mapping its anatomy or measuring its density, but in characterising its behaviour mathematically. \n",
    "- His view is still shared by many. \n",
    "- Mathematical equations are expressive and unambiguous — and arguably the only language suited to describing so complex an object\n",
    "\n",
    "![alt text](https://www.bu.edu/research/files/2015/09/h_research_brain_rhythms_seizure.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- Hearing aids and prosthetic limbs exploit algorithms that mimic the equations of the brain and nervous system\n",
    "- Mathematical characterisations of the brain inform the field of AI and the design of novel computer components\n",
    "\n",
    "![alt text](https://i0.wp.com/www.themanitoban.com/wp-content/uploads/2015/03/Aldo-Rios-Science-turing-brain-Web.jpg?resize=777%2C437 \"Logo Title Text 1\")\n",
    "\n",
    "- Wrote a short computer program that accepted a single number, performed a series of unspecified calculations on it, and returned a second number. \n",
    "- He argued that the brain accept tens-of-thousands of inputs from sensory receptors around the body, but the calculations these inputs undergo are far more complicated than anything written by a single programmer. \n",
    "- He underscored his argument with a wager: that it would take an investigator at least a thousand years to guess the full set of calculations his Manchester program employed. \n",
    "- Guessing the full set of calculations taking place in the brain, he noted, would appear prohibitively time-consuming\n",
    "- With modern supercomputers and neuroimaging techniques, this reduces the complexity orders of magnitude\n",
    "\n",
    "#### McCulloch and Pitts\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/neuralnets-110906041700-phpapp02/95/neural-networks-6-728.jpg?cb=1315282683 \"Logo Title Text 1\")\n",
    "\n",
    "- Both tried to understand how the brain could produce highly complex patterns by using many basic cells that are connected together\n",
    "\n",
    "#### Hopfield\n",
    "\n",
    "![alt text](http://images.slideplayer.com/15/4758037/slides/slide_2.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- Combined McCulloch Pitts Neurons into a Hopfield Network\n",
    "\n",
    "#### Hinton \n",
    "\n",
    "![alt text](https://sebastianraschka.com/images/faq/visual-backpropagation/backpropagation.png \"Logo Title Text 1\")\n",
    "\n",
    "- Invented backpropagation\n",
    "- The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output \n",
    "- Big proponent of Dropout, Non-linearities, Layered Networks during AI winter\n",
    "\n",
    "#### Leading up until now\n",
    "\n",
    "![alt text](https://docs.opencv.org/2.4/_images/optimal-hyperplane.png \"Logo Title Text 1\")\n",
    "\n",
    "- More recently, the interaction between neuroscience an AI has become much less common\n",
    "-  Both subjects have grown enormously in complexity and disciplinary boundaries have solidified.\n",
    "- This is a problem, the human brain is the only existing proof that such an intelligence is even possible. \n",
    "- Neuroscience provides a rich source of inspiration for new types of algorithms and architectures, independent of and complementary to the mathematical and logic-based methods and ideas that have largely dominated traditional approaches to AI.\n",
    "- neuroscience can also  provide validation of AI techniques that already exist.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1200/0*emXbYoppjY65lceR.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- Biological plausibility is a guide, not a strict requirement.\n",
    "- We need a systems neuroscience-level understanding of the brain, namely the algorithms, architectures, functions, and representations it utilizes.\n",
    "- By focusing on the computational and algorithmic levels, we can gain transferrable insights into general mechanisms of brain function, while leaving room to accommodate the distinctive opportunities and challenges that arise when building intelligent machines in silico.\n",
    "\n",
    "\n",
    "## Contemporary Neuroscience in AI (4 Key Areas Affected)\n",
    "\n",
    "### Attention\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/vmvpD.jpg \"Logo Title Text 1\")\n",
    "\n",
    "-  The brain does not learn by implementing a single, global optimization principle within a uniform and undifferentiated neural network. \n",
    "- Rather, biological brains are modular, with distinct but interacting subsystems underpinning key functions such as memory, language, and cognitive control\n",
    "\n",
    "![alt text](http://kelvinxu.github.io/projects/diags/model_diag.png \"Logo Title Text 1\")\n",
    "\n",
    "-  Up until quite lately, most CNN models worked directly on entire images or video frames, with equal priority given to all image pixels at the earliest stage of processing. \n",
    "- The primate visual system works differently. Rather than processing all input in parallel, visual attention shifts strategically among locations and objects, centering processing resources and representational coordinates on a series of regions in turn\n",
    "\n",
    "### Episodic Memory\n",
    "\n",
    "![alt text](https://primarytimerydotcom.files.wordpress.com/2017/09/semantic-episodic-slide.png?w=660 \"Logo Title Text 1\")\n",
    "\n",
    "- A canonical theme in neuroscience is that that intelligent behavior relies on multiple memory systems\n",
    "-  Reinforcement based mechanisms (incremental) and instance based mechanisms (one shot)\n",
    "- Episodic memory is associated with circuits in the medial temporal lobe\n",
    "\n",
    "![alt text](https://reading-club.github.io/assets/posts/Playing_Atari_with_Deep_Reinforcement_Learning/algorithm.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://dnddnjs.gitbooks.io/rl/content/dqn16.png \"Logo Title Text 1\")\n",
    "\n",
    "- DeepMind's Deep Q Network was inspired by it, using  ‘experience replay', where  the network stores a subset of the training data in an instance-based way, and then ‘‘replays’’ it offline, \n",
    "- It learns anew from successes or failures that occurred in the past.\n",
    "- Experience replay is critical to maximizing data efficiency, destabilizing effects of learning from consecutive correlated ex- periences, and allows the network to learn a viable value function even in complex, highly structured sequential environments such as video games.\n",
    "\n",
    "### Working Memory\n",
    "\n",
    "![alt text](http://examinedexistence.com//wp-content/uploads/2013/12/working-memory-2.gif \"Logo Title Text 1\")\n",
    "\n",
    "- Human intelligence can maintain and manipulate information within an active store known as working memory\n",
    "- This is thought to be instantiated within the preforntal cortex and interconnected areas\n",
    "- Classic theories suggest this depends on interactions between a central controller and  speerate domain specific memory buffers\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/dlcvd2l6recurrentneuralnetworks-160802094750/95/deep-learning-for-computer-vision-recurrent-neural-networks-upc-2016-26-638.jpg?cb=1470131837 \"Logo Title Text 1\")\n",
    "\n",
    "- LSTM networks were on step towards implementing this\n",
    "- In LSTMs the functions of sequence control and memory storage are closely intertwined\n",
    "- But this contrasts with classic models of human working memory, which, separate these two.\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-08b667f9a7706e0cf2baf70cd00450b4 \"Logo Title Text 1\")\n",
    "\n",
    "- A possible improvement, the differential neural computer (DNC) involves a neural network controller that attends to and reads/writes from an external memory matrix\n",
    "-  This externalization allows the network controller to learn from scratch (i.e., via end-to-end optimization) to perform a wide range of complex memory and reasoning tasks that currently elude LSTMs, such as finding the shortest path through a graph-like structure, such as a subway map, or manipulating blocks in a variant of the Tower of Hanoi task\n",
    "\n",
    "### Continual Learning\n",
    "\n",
    "![alt text](https://creativesystemsthinking.files.wordpress.com/2014/02/971854_575795465785620_1014001935_n.jpg?w=650 \"Logo Title Text 1\")\n",
    "\n",
    "-  Intelligent agents must be able to learn and remember many different tasks that are encountered over multiple timescales.\n",
    "- Continual Learning is an ability to master new tasks without forgetting how to perform prior tasks \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/overcomingcatastrophicforgettinginneuralnetwork-170429024916/95/overcoming-catastrophic-forgetting-in-neural-network-3-638.jpg?cb=1493434190 \"Logo Title Text 1\")\n",
    "\n",
    "- While animals appear relatively adept at continual learning, neural networks suffer from the problem of catastrophic forgetting \n",
    "- This occurs as the network parameters shift toward the optimal state for performing the second of two successive tasks, overwriting the configuration that allowed them to perform the first. \n",
    "\n",
    "![alt text](https://s3-eu-west-1.amazonaws.com/ppreviews-plos-725668748/1876046/preview.jpg \"Logo Title Text 1\")\n",
    "\n",
    "-  Advanced neuroimaging techniques (e.g., two-photon imaging) now allow dynamic in vivo visualization of the structure and function of dendritic spines during learning, at the spatial scale of single synapses\n",
    "- This approach can be used to study neocortical plasticity during continual learning\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/overcomingcatastrophicforgettinginneuralnetwork-170429024916/95/overcoming-catastrophic-forgetting-in-neural-network-9-638.jpg?cb=1493434190 \"Logo Title Text 1\")\n",
    "\n",
    "- Elastic Weight Consolidtion is one newer step in this direction\n",
    "- acts by slowing down learning in a subset of network weights identified as important to previous tasks, thereby anchoring these parameters to previously found solutions. \n",
    "- This allows multiple tasks to be learned without an increase in network capacity, with weights shared efficiently between tasks with related structure. \n",
    "\n",
    "## The Future (5 Areas of AI Neuroscience will Improve)\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-67616bfe157abcb81f86aab60ddfc50e \"Logo Title Text 1\")\n",
    "\n",
    "- The advent of new tools for brain imaging and genetic bioengineering have begun to offer a detailed characterization of the computations occurring in neural circuits\n",
    "- This will revolutionize our understanding of mammalian brain function\n",
    "\n",
    "## Understanding of Physical reality \n",
    "\n",
    "![alt text](https://i2.wp.com/neurosciencenews.com/files/2018/01/baby-touch-learning-neurosciencenews.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- Human Infants have knowledge of core concepts relating to the physical world, such as space, number, and objectness, which allow them to construct compositional mental models that can guide inference and prediction \n",
    "\n",
    "![alt text](https://adriancolyer.files.wordpress.com/2016/12/img_0097.jpeg?w=600 \"Logo Title Text 1\")\n",
    "\n",
    "- Novel neural network architectures have been developed that interpret and reason about scenes in a humanlike way, by decomposing them into individual objects and their relations\n",
    "- Deep RL has been used to capture the processes by which children gain commonsense understanding of the world through interactive experiments\n",
    "\n",
    "## Efficient learning\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*jkiFPzRue7h7z1SjUCEarA.png \"Logo Title Text 1\")\n",
    "\n",
    "- Human cognition is distinguished by its ability to rapidly learn about new concepts from only a handful of examples\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-a078ed29b350b9b2b1e12c3560dced70-c \"Logo Title Text 1\")\n",
    "\n",
    "- Humans excel at generalizing or transferring generalized knowledge gained in one context to novel, previously unseen domains\n",
    "- one recent report made the very interesting claim that neural codes thought to be important in the representation of map-like spaces might be critical for abstract reasoning in more general domains (Constantinescu et al., 2016). \n",
    "\n",
    "![alt text](http://nmr.mgh.harvard.edu/mkozhevnlab/wp-content/uploads/images/rp/Spatial_Coding_Systems.png \"Logo Title Text 1\")\n",
    "\n",
    "- In the mammalian cortex, cells encode the geometry of map-like space with a periodic ‘‘grid’’ code, with receptive fields that tile the local space in a hexagonal pattern\n",
    "- Grid codes may be an excellent candidate for organizing conceptual knowledge, because they allow state spaces to be decomposed efficiently, in a way that could support discovery of subgoals and hierarchical planning \n",
    "- Using functional neuroimaging, the researchers provided evidence for the existence of such codes while humans performed an abstract categorization task, supporting the view that periodic encoding is a generalized hallmark of human knowledge organization (Constantinescu et al., 2016). \n",
    "\n",
    "## Imagination and Planning\n",
    "\n",
    "![alt text](https://rubenfiszel.github.io/posts/rl4j/conv.png \"Logo Title Text 1\")\n",
    "\n",
    "- Despite their strong performance on goal-directed tasks, deep RL systems such as DQN operate mostly in a reactive way, learning the mapping from perceptual inputs to actions that maximize future value. \n",
    "- Two major drawbacks: it is relatively data inefficient, requiring large amounts of experience to derive accurate estimates\n",
    "- and it is inflexible, being insensitive to changes in the value of outcomes\n",
    "\n",
    "https://arxiv.org/abs/1707.06203\n",
    "\n",
    "\n",
    "![alt text](https://pbs.twimg.com/media/DFKIqmUW0AAXj_N.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- By contrast, humans can more flexibly select actions based on forecasts of long-term future outcomes through simulation-based planning, which uses predictions generated from an internal model of the environment learned through experience\n",
    "- An emerging picture from neuroscience research suggests that the hippo- campus supports planning by instantiating an internal model of the environment, with goal-contingent valuation of simulated outcomes occurring in areas downstream of the hippocampus (Redish, 2016)\n",
    "- AI research will benefit from a close reading of the related literature on how humans imagine possible scenarios, envision the future, and carry out simulation- based planning, functions that depend on a common neural substrate in the hippocampus\n",
    "- Research into human imagination emphasizes its constructive nature, with humans able to construct fictitious mental scenarios by recombining familiar elements in novel ways, necessitating compositional/disentangled representations of the form present in certain generative models\n",
    "\n",
    "## Virtual Brain Analytics\n",
    "\n",
    "![alt text](http://www.cfhu.org/sites/files/cfhu.org/quanta-magazine-header-deep-learning.gif \"Logo Title Text 1\")\n",
    "\n",
    "- Due to their complexity, the products of AI research often remain ‘‘black boxes’’; \n",
    "- we understand only poorly the nature of the computations that occur, or representations that are formed, during learning of complex tasks.\n",
    "\n",
    "![alt text](http://simpleneuroscience.com/wp-content/uploads/2017/03/800px-Two-photon_microscopy_of_in_vivo_brain_function-777x437.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- But by applying tools from neuroscience to AI systems, synthetic equiv- alents of single-cell recording, neuroimaging, and lesion techniques, we can gain insights into the key drivers of successful learning in AI research and increase the interpretability of these systems. i.e ‘‘virtual brain analytics.'\n",
    "- visualizing brain states through dimensionality reduction is commonplace in neuroscience, can be applied more in AI research\n",
    "- Important as complexity of neural architecture increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
